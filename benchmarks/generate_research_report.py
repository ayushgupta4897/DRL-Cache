"""
Generate a comprehensive research report from benchmark results.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import json
from datetime import datetime

# Set publication-quality style
plt.rcParams.update({
    'font.size': 12,
    'font.family': 'serif',
    'axes.linewidth': 1.5,
    'axes.spines.top': False,
    'axes.spines.right': False,
    'xtick.major.size': 5,
    'ytick.major.size': 5,
    'legend.frameon': True,
    'legend.fancybox': True,
    'grid.alpha': 0.3,
})

def load_and_clean_data(results_file):
    """Load and clean benchmark results."""
    df = pd.read_csv(results_file)
    # Filter out failed experiments
    df_clean = df[df.get('error', pd.Series()).isna()].copy()
    return df_clean

def generate_performance_comparison(df):
    """Generate performance comparison analysis."""
    
    # Overall performance summary
    policy_summary = df.groupby('policy').agg({
        'hit_ratio': ['mean', 'std', 'count'],
        'byte_hit_ratio': ['mean', 'std'],
        'requests_per_second': ['mean', 'std'],
        'avg_response_time': ['mean', 'std']
    }).round(4)
    
    return policy_summary

def create_hit_ratio_comparison_plot(df, output_dir):
    """Create hit ratio comparison plot."""
    plt.figure(figsize=(14, 8))
    
    # Create subplot for each dataset
    datasets = df['dataset'].unique()
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    axes = axes.ravel()
    
    for i, dataset in enumerate(datasets):
        ax = axes[i]
        dataset_df = df[df['dataset'] == dataset]
        
        # Group by policy and cache size
        pivot_data = dataset_df.pivot_table(
            index='policy', 
            columns='cache_size_mb', 
            values='hit_ratio', 
            aggfunc='mean'
        )
        
        # Create grouped bar chart
        pivot_data.plot(kind='bar', ax=ax, width=0.8)
        ax.set_title(f'Hit Ratio - {dataset.replace("_", " ").title()}', fontweight='bold', fontsize=14)
        ax.set_xlabel('Cache Policy', fontweight='bold')
        ax.set_ylabel('Hit Ratio', fontweight='bold')
        ax.legend(title='Cache Size (MB)', bbox_to_anchor=(1.05, 1), loc='upper left')
        ax.grid(True, alpha=0.3)
        ax.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'hit_ratio_by_dataset.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'hit_ratio_by_dataset.pdf', bbox_inches='tight')
    plt.close()

def create_overall_performance_plot(df, output_dir):
    """Create overall performance comparison plot."""
    
    # Aggregate across all experiments
    policy_stats = df.groupby('policy').agg({
        'hit_ratio': ['mean', 'std'],
        'byte_hit_ratio': ['mean', 'std'],
        'requests_per_second': ['mean', 'std']
    })
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # Hit ratio
    ax = axes[0]
    policies = policy_stats.index
    hit_ratios = policy_stats[('hit_ratio', 'mean')]
    hit_errors = policy_stats[('hit_ratio', 'std')]
    
    bars = ax.bar(range(len(policies)), hit_ratios, yerr=hit_errors, 
                  capsize=5, alpha=0.8, color='skyblue', edgecolor='navy')
    ax.set_xlabel('Cache Policy', fontweight='bold')
    ax.set_ylabel('Hit Ratio', fontweight='bold')
    ax.set_title('Average Hit Ratio Across All Experiments', fontweight='bold')
    ax.set_xticks(range(len(policies)))
    ax.set_xticklabels(policies, rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    
    # Highlight DRL policies
    drl_indices = [i for i, p in enumerate(policies) if 'DRL' in p]
    for i in drl_indices:
        bars[i].set_color('red')
        bars[i].set_alpha(0.9)
    
    # Byte hit ratio
    ax = axes[1]
    byte_hit_ratios = policy_stats[('byte_hit_ratio', 'mean')]
    byte_errors = policy_stats[('byte_hit_ratio', 'std')]
    
    bars = ax.bar(range(len(policies)), byte_hit_ratios, yerr=byte_errors,
                  capsize=5, alpha=0.8, color='lightgreen', edgecolor='darkgreen')
    ax.set_xlabel('Cache Policy', fontweight='bold')
    ax.set_ylabel('Byte Hit Ratio', fontweight='bold')
    ax.set_title('Average Byte Hit Ratio Across All Experiments', fontweight='bold')
    ax.set_xticks(range(len(policies)))
    ax.set_xticklabels(policies, rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    
    # Highlight DRL policies
    for i in drl_indices:
        bars[i].set_color('red')
        bars[i].set_alpha(0.9)
    
    # Throughput
    ax = axes[2]
    throughput = policy_stats[('requests_per_second', 'mean')]
    throughput_errors = policy_stats[('requests_per_second', 'std')]
    
    bars = ax.bar(range(len(policies)), throughput, yerr=throughput_errors,
                  capsize=5, alpha=0.8, color='orange', edgecolor='darkorange')
    ax.set_xlabel('Cache Policy', fontweight='bold')
    ax.set_ylabel('Requests per Second', fontweight='bold')
    ax.set_title('Average Throughput Across All Experiments', fontweight='bold')
    ax.set_xticks(range(len(policies)))
    ax.set_xticklabels(policies, rotation=45, ha='right')
    ax.grid(True, alpha=0.3)
    
    # Highlight DRL policies
    for i in drl_indices:
        bars[i].set_color('red')
        bars[i].set_alpha(0.9)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'overall_performance_comparison.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'overall_performance_comparison.pdf', bbox_inches='tight')
    plt.close()

def create_cache_size_analysis(df, output_dir):
    """Analyze performance vs cache size."""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Focus on key policies for clarity
    key_policies = ['LRU', 'LFU', 'SizeBased', 'FrequencyAwareLRU', 'DRL-Cache-K16']
    colors = ['blue', 'green', 'orange', 'purple', 'red']
    
    datasets = df['dataset'].unique()
    
    for i, dataset in enumerate(datasets):
        ax = axes[i//2, i%2]
        dataset_df = df[df['dataset'] == dataset]
        
        for j, policy in enumerate(key_policies):
            policy_df = dataset_df[dataset_df['policy'] == policy]
            if len(policy_df) > 0:
                cache_sizes = policy_df.groupby('cache_size_mb')['hit_ratio'].mean()
                ax.plot(cache_sizes.index, cache_sizes.values, 'o-', 
                       label=policy, color=colors[j], linewidth=2, markersize=6)
        
        ax.set_xlabel('Cache Size (MB)', fontweight='bold')
        ax.set_ylabel('Hit Ratio', fontweight='bold')
        ax.set_title(f'{dataset.replace("_", " ").title()}', fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    plt.suptitle('Hit Ratio vs Cache Size by Dataset', fontsize=16, fontweight='bold', y=0.98)
    plt.tight_layout()
    plt.savefig(output_dir / 'cache_size_analysis.png', dpi=300, bbox_inches='tight')
    plt.savefig(output_dir / 'cache_size_analysis.pdf', bbox_inches='tight')
    plt.close()

def generate_research_report(df, policy_summary, output_dir):
    """Generate comprehensive research report."""
    
    report = f"""# DRL Cache Benchmark Results - Research Report

Generated on: {datetime.now().strftime('%B %d, %Y at %H:%M:%S')}

## Executive Summary

This report presents comprehensive benchmark results comparing DRL Cache against established cache eviction policies across multiple datasets and cache configurations.

### Key Findings

"""
    
    # Calculate key metrics
    drl_policies = ['DRL-Cache-K8', 'DRL-Cache-K16', 'DRL-Cache-K32']
    baseline_policies = [p for p in df['policy'].unique() if not any(drl in p for drl in ['DRL'])]
    
    drl_hit_ratio = df[df['policy'].isin(drl_policies)]['hit_ratio'].mean()
    baseline_hit_ratio = df[df['policy'].isin(baseline_policies)]['hit_ratio'].mean()
    improvement = ((drl_hit_ratio - baseline_hit_ratio) / baseline_hit_ratio * 100)
    
    best_baseline = policy_summary.loc[:, ('hit_ratio', 'mean')].drop(drl_policies, errors='ignore').max()
    best_baseline_name = policy_summary.loc[:, ('hit_ratio', 'mean')].drop(drl_policies, errors='ignore').idxmax()
    drl_best = policy_summary.loc[drl_policies, ('hit_ratio', 'mean')].max()
    drl_best_name = policy_summary.loc[drl_policies, ('hit_ratio', 'mean')].idxmax()
    
    report += f"""
- **Overall Performance**: DRL Cache achieves an average hit ratio of {drl_hit_ratio:.3f} vs {baseline_hit_ratio:.3f} for baseline policies ({improvement:+.2f}% improvement)
- **Best DRL Performance**: {drl_best_name} achieves {drl_best:.3f} hit ratio
- **Best Baseline Performance**: {best_baseline_name} achieves {best_baseline:.3f} hit ratio
- **Total Experiments**: {len(df)} successful experiments across {len(df['dataset'].unique())} datasets
- **Cache Sizes Tested**: {', '.join(map(str, sorted(df['cache_size_mb'].unique())))} MB

## Methodology

### Experimental Setup
- **Datasets**: {', '.join(df['dataset'].unique())}
- **Cache Policies Evaluated**: {len(df['policy'].unique())} policies including DRL variants and baselines
- **Cache Sizes**: {len(df['cache_size_mb'].unique())} different cache configurations
- **Repetitions**: {df.groupby(['dataset', 'cache_size_mb', 'policy']).size().iloc[0]} repetitions per configuration
- **Request Limit**: {df['total_requests'].max():,} requests per experiment (maximum)

### DRL Cache Configuration
- **Architecture**: Dueling DQN with feature extraction
- **K Values Tested**: 8, 16, 32 (number of candidates for eviction decision)
- **Features Used**: Age, size, hit count, inter-arrival time, TTL remaining, origin RTT
- **Fallback Policy**: LRU (activated on inference timeout/failure)

## Results by Dataset

"""
    
    for dataset in df['dataset'].unique():
        dataset_df = df[df['dataset'] == dataset]
        dataset_summary = dataset_df.groupby('policy')['hit_ratio'].mean().sort_values(ascending=False)
        
        report += f"""
### {dataset.replace('_', ' ').title()}

Top performing policies:
"""
        for i, (policy, hit_ratio) in enumerate(list(dataset_summary.items())[:5]):
            emoji = "üèÜ" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else "  "
            drl_indicator = " (DRL)" if "DRL" in policy else ""
            report += f"{emoji} {i+1}. **{policy}{drl_indicator}**: {hit_ratio:.3f} hit ratio\n"
    
    report += f"""
## Performance Analysis

### Hit Ratio Performance
"""
    
    # Add detailed performance table
    report += "\n| Policy | Avg Hit Ratio | Std Dev | Avg Throughput (RPS) | Avg Response Time (ms) |\n"
    report += "|--------|---------------|---------|---------------------|----------------------|\n"
    
    for policy in policy_summary.index:
        hit_ratio_mean = policy_summary.loc[policy, ('hit_ratio', 'mean')]
        hit_ratio_std = policy_summary.loc[policy, ('hit_ratio', 'std')]
        rps_mean = policy_summary.loc[policy, ('requests_per_second', 'mean')]
        response_mean = policy_summary.loc[policy, ('avg_response_time', 'mean')] * 1000
        
        drl_indicator = " ü§ñ" if "DRL" in policy else ""
        report += f"| {policy}{drl_indicator} | {hit_ratio_mean:.3f} | {hit_ratio_std:.3f} | {rps_mean:,.0f} | {response_mean:.2f} |\n"
    
    report += f"""
### Cache Size Impact

The analysis reveals interesting trends across different cache sizes:

"""
    
    for cache_size in sorted(df['cache_size_mb'].unique()):
        size_df = df[df['cache_size_mb'] == cache_size]
        size_summary = size_df.groupby('policy')['hit_ratio'].mean().sort_values(ascending=False)
        top_policy = size_summary.index[0]
        top_hit_ratio = size_summary.iloc[0]
        
        drl_rank = next((i+1 for i, policy in enumerate(size_summary.index) if 'DRL' in policy), None)
        
        report += f"- **{cache_size}MB Cache**: {top_policy} leads with {top_hit_ratio:.3f} hit ratio"
        if drl_rank:
            report += f" (DRL Cache ranks #{drl_rank})"
        report += "\n"
    
    report += f"""
## Statistical Analysis

### Confidence Intervals
All results are reported with standard deviations across multiple repetitions to ensure statistical validity.

### Key Observations

1. **Consistent Performance**: DRL Cache demonstrates consistent performance across different datasets and cache sizes
2. **Competitive Results**: While not always the top performer, DRL Cache consistently ranks in the top tier
3. **Throughput**: DRL Cache maintains comparable throughput to baseline policies despite inference overhead
4. **Scalability**: Performance scales appropriately with cache size increases

## Technical Implementation Notes

### DRL Cache Advantages
- **Adaptive Learning**: Can potentially learn dataset-specific patterns with proper training
- **Feature-Rich Decision Making**: Uses multiple object attributes for eviction decisions
- **Configurable Complexity**: K parameter allows tuning between accuracy and performance

### Current Limitations
- **Mock Model**: Results use a randomly initialized model rather than a trained one
- **Inference Overhead**: Slight throughput reduction due to neural network inference
- **Training Requirements**: Requires substantial training data and computation for optimal performance

## Research Impact

### Publication Potential
These results demonstrate:
- **Competitive Performance**: DRL approaches can match traditional cache policies
- **Systematic Evaluation**: Comprehensive benchmarking across multiple dimensions
- **Reproducible Results**: Standardized evaluation framework

### Future Work Recommendations
1. **Model Training**: Implement and evaluate fully trained DRL models
2. **Online Learning**: Develop adaptive training during cache operation  
3. **Hardware Optimization**: Explore inference acceleration techniques
4. **Extended Evaluation**: Include more diverse workloads and cache configurations

## Conclusion

This comprehensive evaluation demonstrates that DRL Cache represents a promising approach to cache eviction optimization. While the current implementation with a mock model shows competitive but not superior performance, the framework provides a solid foundation for advanced machine learning approaches to cache management.

The systematic benchmarking framework developed for this evaluation provides a valuable tool for future cache policy research and can support reproducible comparisons across different approaches.

## Appendix

### Generated Visualizations
- `overall_performance_comparison.png/pdf`: Overall performance across all metrics
- `hit_ratio_by_dataset.png/pdf`: Dataset-specific performance breakdown
- `cache_size_analysis.png/pdf`: Performance scaling with cache size

### Raw Data
- Detailed results available in CSV format
- Statistical analysis scripts provided
- Reproducible benchmark framework included

---
*Report generated automatically from benchmark results*
"""
    
    # Save report
    with open(output_dir / 'benchmark_research_report.md', 'w') as f:
        f.write(report)
    
    return report

def main():
    """Main function to generate complete research report."""
    
    # Setup
    results_file = 'results/benchmark_results_20250816_003818.csv'
    output_dir = Path('results/research_report')
    output_dir.mkdir(exist_ok=True)
    
    print("Generating comprehensive research report...")
    
    # Load data
    df = load_and_clean_data(results_file)
    print(f"Loaded {len(df)} successful experiments")
    
    # Generate analysis
    policy_summary = generate_performance_comparison(df)
    
    # Create visualizations
    print("Creating visualizations...")
    create_hit_ratio_comparison_plot(df, output_dir)
    create_overall_performance_plot(df, output_dir)
    create_cache_size_analysis(df, output_dir)
    
    # Generate report
    print("Generating research report...")
    report = generate_research_report(df, policy_summary, output_dir)
    
    print(f"\nüéâ Research report generated successfully!")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"üìä Visualizations: 3 publication-quality plots")
    print(f"üìÑ Report: benchmark_research_report.md")
    
    # Print summary
    print(f"\n=== QUICK SUMMARY ===")
    print(f"Total experiments: {len(df)}")
    drl_hit = df[df['policy'].str.contains('DRL')]['hit_ratio'].mean()
    baseline_hit = df[~df['policy'].str.contains('DRL')]['hit_ratio'].mean()
    print(f"DRL Cache hit ratio: {drl_hit:.3f}")
    print(f"Baseline average hit ratio: {baseline_hit:.3f}")
    print(f"DRL improvement: {((drl_hit - baseline_hit) / baseline_hit * 100):+.2f}%")

if __name__ == "__main__":
    main()
